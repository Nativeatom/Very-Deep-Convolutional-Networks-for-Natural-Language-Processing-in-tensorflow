{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "samples = {}\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\"\n",
    "FEATURE_LEN =1014\n",
    "cdict = {}\n",
    "for i,c in enumerate(ALPHABET):\n",
    "    cdict[c] = i + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = {}\n",
    "with open('/dbpedia_data/train.csv') as f:\n",
    "    reader = csv.DictReader(f,fieldnames=['class'],restkey='fields')\n",
    "    for row in reader:\n",
    "        label = row['class']\n",
    "        if label not in samples:\n",
    "            samples[label] = []\n",
    "        sample = np.ones(FEATURE_LEN)\n",
    "        count = 0\n",
    "        for field in row['fields']:\n",
    "            for char in field.lower():\n",
    "                if char in cdict:\n",
    "                    sample[count] = cdict[char]\n",
    "                    count += 1\n",
    "                if count >= FEATURE_LEN-1:\n",
    "                    break\n",
    "        samples[label].append(sample)\n",
    "    samples_per_class = None\n",
    "    classes = samples.keys()\n",
    "    class_samples = []\n",
    "    for c in classes:\n",
    "        if samples_per_class is None:\n",
    "            samples_per_class = len(samples[c])\n",
    "        else:\n",
    "            assert samples_per_class == len(samples[c])\n",
    "        class_samples.append(samples[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_onehot(input_):\n",
    "    target = np.zeros(len(classes))\n",
    "    target[input_] = 1\n",
    "    return target\n",
    "y= []\n",
    "for i in range(len(classes)):\n",
    "    for j in range(samples_per_class):\n",
    "        target =build_onehot(i)\n",
    "        y.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.reshape(class_samples,(-1,FEATURE_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.reshape(y,(-1,len(classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "# Split train/test set\n",
    "n_dev_samples = 10000\n",
    "# TODO: Create a fuckin' correct cross validation procedure\n",
    "x_train, x_dev = x_shuffled[:-n_dev_samples], x_shuffled[-n_dev_samples:]\n",
    "y_train, y_dev = y_shuffled[:-n_dev_samples], y_shuffled[-n_dev_samples:]\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Convolutional_Block(input_,filter_num,train,scope):\n",
    "    norm = tf.random_normal_initializer(stddev=0.05)\n",
    "    const = tf.constant_initializer(0.0)\n",
    "    filter_shape1 = [3, 1, input_.get_shape()[3], filter_num]\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        filter_1 = tf.get_variable('filter1', filter_shape1, initializer=norm)\n",
    "        conv1 = tf.nn.conv2d(input_, filter_1, strides=[1, 1, filter_shape1[1], 1], padding=\"SAME\")\n",
    "        batch_normal1 =tflearn.layers.normalization.batch_normalization(conv1,trainable= train,scope = scope+\"BN1\")\n",
    "        filter_shape2 = [3, 1, batch_normal1.get_shape()[3], filter_num]\n",
    "        filter_2 = tf.get_variable('fileer2', filter_shape2, initializer=norm)\n",
    "        conv2 = tf.nn.conv2d(tf.nn.relu(batch_normal1), filter_2, strides=[1, 1, filter_shape2[1], 1], padding=\"SAME\")\n",
    "        batch_normal2 =tflearn.layers.normalization.batch_normalization(conv2,trainable=train,scope = scope+\"BN2\")\n",
    "        pooled = tf.nn.max_pool(tf.nn.relu(batch_normal2),ksize=[1, 3, 1, 1],strides=[1, 2, 1, 1],padding='SAME',name=\"pool1\")\n",
    "        return pooled\n",
    "def Conv(input_,filter_shape,strides,train,scope):\n",
    "    norm = tf.random_normal_initializer(stddev=0.05)\n",
    "    const = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope(scope):\n",
    "        filter_1 = tf.get_variable('filter1', filter_shape, initializer=norm)\n",
    "        conv = tf.nn.conv2d(input_, filter_1, strides=strides, padding=\"SAME\")\n",
    "        batch_normal =tflearn.layers.normalization.batch_normalization(conv,trainable=train,scope = scope+\"BN\")\n",
    "        return batch_normal\n",
    "def linear(input, output_dim, scope=None, stddev=0.1):\n",
    "    norm = tf.random_normal_initializer(stddev=stddev)\n",
    "    const = tf.constant_initializer(0.0)\n",
    "    with tf.variable_scope(scope or 'linear'):\n",
    "        w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm)\n",
    "        b = tf.get_variable('b', [output_dim], initializer=const)\n",
    "        l2_loss = tf.nn.l2_loss(w)+tf.nn.l2_loss(b)\n",
    "        return tf.matmul(input, w) + b,l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from inception.slim import ops\n",
    "import tflearn\n",
    "class CharCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    based on the Very Deep Convolutional Networks for Natural Language Processing.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=14, filter_size=3,\n",
    "                 l2_reg_lambda=0.001, sequence_max_length=1014, num_quantized_chars=71,embedding_size=16):\n",
    "        \n",
    "        self.input_x = tf.placeholder(tf.int32, [None,sequence_max_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        self.training =  tf.placeholder(tf.int32, name=\"trainable\")\n",
    "        if self.training==1:\n",
    "            TRAIN = True\n",
    "        else:\n",
    "            TRAIN = False\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        with tf.device('/cpu:0'),tf.name_scope(\"embedding\"):\n",
    "            W0 = tf.Variable(tf.random_uniform([num_quantized_chars, embedding_size], -1.0, 1.0),name=\"W\")\n",
    "            self.embedded_characters = tf.nn.embedding_lookup(W0,self.input_x)\n",
    "            self.embedded_characters_expanded = tf.expand_dims(self.embedded_characters,-1,name=\"embedding_input\")\n",
    "        with tf.name_scope(\"layer-0\"): \n",
    "            filter_shape0 = [3, embedding_size, 1, 64]\n",
    "            strides0 =[1, 1,embedding_size, 1]\n",
    "            self.h0 = Conv(self.embedded_characters_expanded,filter_shape0,strides0,TRAIN,'layer_0')\n",
    "        with tf.name_scope(\"layer-1-2-3-4-5-6-7-8\"):\n",
    "            self.h1 = Convolutional_Block(self.h0,64,TRAIN,'layer_1-2')\n",
    "            self.h2 = Convolutional_Block(self.h1,128,TRAIN,'layer_3-4')\n",
    "            self.h3 = Convolutional_Block(self.h2,256,TRAIN,'layer_5-6')\n",
    "            self.h4 = Convolutional_Block(self.h3,512,TRAIN,'layer_7-8')\n",
    "            self.h5 = tf.transpose(self.h4,[0,3,2,1])\n",
    "            self.pooled = tf.nn.top_k(self.h5, k=8,name='k-maxpooling') \n",
    "            self.h6 = tf.reshape(self.pooled[0],(-1,512*8))\n",
    "        with tf.name_scope(\"fc-1-2-3\"):\n",
    "            self.fc1_out,fc1_loss = linear(self.h6, 2048, scope='fc1', stddev=0.1)\n",
    "            l2_loss += fc1_loss\n",
    "            self.fc2_out,fc2_loss = linear(tf.nn.relu(self.fc1_out), 2048, scope='fc2', stddev=0.1)\n",
    "            l2_loss += fc2_loss\n",
    "            self.fc3_out,fc3_loss = linear(tf.nn.relu(self.fc2_out), num_classes, scope='fc3', stddev=0.1)\n",
    "            l2_loss += fc3_loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.predictions = tf.argmax(self.fc3_out, 1, name=\"predictions\")\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits = self.fc3_out,labels = self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batch = x_train[0:64]\n",
    "y_batch = y_train[0:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ.setdefault('CUDA_VISIBLE_DEVICES','3')\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.7)\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(gpu_options = gpu_options,\n",
    "                                  allow_soft_placement=True,\n",
    "                                  log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = CharCNN()\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        feed_dict = {\n",
    "            cnn.input_x: x_batch,\n",
    "            cnn.input_y: y_batch,\n",
    "            cnn.training:1,\n",
    "            cnn.dropout_keep_prob: 0.5\n",
    "        }       \n",
    "        out = sess.run(cnn.accuracy,feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "def batch_iter(x, y, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    # data = np.array(data)\n",
    "    data_size = len(x)\n",
    "    num_batches_per_epoch = int(data_size/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"In epoch >> \" + str(epoch + 1))\n",
    "        print(\"num batches per epoch is: \" + str(num_batches_per_epoch))\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            x_shuffled = x[shuffle_indices]\n",
    "            y_shuffled = y[shuffle_indices]\n",
    "        else:\n",
    "            x_shuffled = x\n",
    "            y_shuffled = y\n",
    "        for batch_num in range(num_batches_per_epoch-1):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            x_batch = x_shuffled[start_index:end_index]\n",
    "            y_batch = y_shuffled[start_index:end_index]\n",
    "            batch = list(zip(x_batch, y_batch))\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size =128\n",
    "num_epochs = 20\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = CharCNN()\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(\n",
    "            grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\n",
    "                    \"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\n",
    "                    \"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(\n",
    "            os.path.curdir, \"runs_new\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge(\n",
    "            [loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(\n",
    "            train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already\n",
    "        # exists, so we need to create it.\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.training:1,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }       \n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op,\n",
    "                    cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            # write fewer training summaries, to keep events file from\n",
    "            # growing so big.\n",
    "            if step % (evaluate_every / 2) == 0:\n",
    "                print(\"{}: step {}, loss {:g}, acc {:g}\".format(\n",
    "                    time_str, step, loss, accuracy))\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            dev_size = len(x_batch)\n",
    "            max_batch_size = 500\n",
    "            num_batches = dev_size/max_batch_size\n",
    "            acc = []\n",
    "            losses = []\n",
    "            print(\"Number of batches in dev set is \" + str(num_batches))\n",
    "            for i in range(num_batches):\n",
    "                x_batch_dev = x_batch[i * max_batch_size:(i + 1) * max_batch_size]\n",
    "                y_batch_dev = y_batch[i * max_batch_size: (i + 1) * max_batch_size]\n",
    "                feed_dict = {\n",
    "                  cnn.input_x: x_batch_dev,\n",
    "                  cnn.input_y: y_batch_dev,\n",
    "                  cnn.training:0,\n",
    "                  cnn.dropout_keep_prob: 1.0\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                    feed_dict)\n",
    "                acc.append(accuracy)\n",
    "                losses.append(loss)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                print(\"batch \" + str(i + 1) + \" in dev >>\" +\" {}: loss {:g}, acc {:g}\".format(time_str, loss, accuracy))\n",
    "                if writer:\n",
    "                    writer.add_summary(summaries, step)\n",
    "            print(\"\\nMean accuracy=\" + str(sum(acc)/len(acc)))\n",
    "            print(\"Mean loss=\" + str(sum(losses)/len(losses)))\n",
    "        # just for epoch counting\n",
    "        num_batches_per_epoch = int(len(x_train)/batch_size) + 1\n",
    "        # Generate batches\n",
    "        batches = batch_iter(x_train, y_train,batch_size, num_epochs)\n",
    "        evaluate_every = 300\n",
    "        checkpoint_every =300\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"Epoch: {}\".format(\n",
    "                    int(current_step / num_batches_per_epoch)))\n",
    "                print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(\n",
    "                    sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
